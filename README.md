# Reinforcement Learning Unveiled: An Interactive Journey

Uma plataforma de aprendizado interativa que desmistifica o Reinforcement Learning atrav√©s de explica√ß√µes visuais e explora√ß√£o pr√°tica. O foco principal √© uma visualiza√ß√£o interativa que permite controlar o processo de aprendizado e ver como o RL funciona utilizando Q-learning.

## Sobre o Projeto

Este projeto foi desenvolvido para tornar o Reinforcement Learning mais acess√≠vel e compreens√≠vel atrav√©s de tr√™s m√≥dulos principais:

1. **Explica√ß√£o B√°sica**: Introdu√ß√£o aos conceitos fundamentais do RL atrav√©s de exemplos interativos
2. **Teoria Formal**: Defini√ß√µes matem√°ticas rigorosas de agentes, ambientes, pol√≠ticas, recompensa e o algoritmo de aprendizado (Q-Learning)
3. **Visualiza√ß√£o Interativa**: Ambiente pr√°tico onde voc√™ pode ajustar par√¢metros, observar o agente aprender e ver os resultados em tempo real

### Objetivo Principal

Combinar explica√ß√µes visuais com explora√ß√£o para facilitar o entendimento de como algoritmos de RL, especificamente Q-learning, funcionam na pr√°tica.

## Funcionalidades da Visualiza√ß√£o

### Ambiente Interativo
- **Grid World Customiz√°vel**: Configure o tamanho do mundo, posi√ß√µes inicial e final, e obst√°culos.
- **Controle de Par√¢metros**: Ajuste taxa de aprendizado (Œ±), fator de desconto (Œ≥), epsilon e decay.
- **Visualiza√ß√£o em Tempo Real**: Observe o agente aprendendo e tomando decis√µes.

### üéõÔ∏è Controles de Reprodu√ß√£o
- **Play/Pause**: Controle a execu√ß√£o da anima√ß√£o.
- **Velocidades M√∫ltiplas**: 4 n√≠veis de velocidade de reprodu√ß√£o.
- **Navega√ß√£o Manual**: V√° para epis√≥dios e passos espec√≠ficos clicando nos gr√°ficos ou na barra deslizante.
- **Controles de Epis√≥dio**: Navegue entre diferentes tentativas do agente.

### Componentes Visuais!

#### Grid do Ambiente
- Visualiza√ß√£o do mundo onde o agente opera
- Posi√ß√µes de in√≠cio, objetivo e obstaculos (fantasmas) claramente marcadas
- Acompanhamento da posi√ß√£o atual do agente ao longo do aprendizado

<div align="center">
  
![ambiente](https://github.com/user-attachments/assets/20beb0a3-f356-4edb-854d-0fd5e642b025)

</div>

#### Sucesso ao longo dos epis√≥dios
- Acompanha a evolu√ß√£o da performance do agente ao longo dos epis√≥dios.
- Mostra como o agente melhora com o tempo.

<div align="center">
  
![sucesso](https://github.com/user-attachments/assets/ef86a183-89ac-4f87-b3db-7efbb0e0e764)

</div>

#### Grid de Q-valores
- Mostra o valor m√°ximo de Q para cada c√©lula
- Representa o maior valor esperado para aquela c√©lula em uma itera√ß√£o espec√≠fica
- Mostra o melhor caminho aprendido claramente

<div align="center">
  
![q-values-grid](https://github.com/user-attachments/assets/615fe881-e1cc-4f59-b20b-23ebf58fceb6)

</div>

#### Grid da Pol√≠tica Aprendida

- Mostra a pol√≠tica aprendida para cada c√©lula atrav√©s de setas direcionais.
- C√©lulas com um ponto de interroga√ß√£o ("?") representam estados que n√£o foram explorados

<div align="center">
  
![politica](https://github.com/user-attachments/assets/42ba0a58-a0b6-4e13-829d-8edb8f1df322)

</div>

#### Gr√°ficos de uma c√©lula espec√≠fica
- **Q-Valor**: Mostra o Q-valor para cada a√ß√£o (esquerda, direita, cima e baixo) para a c√©lula selecionada
- **Q-Valor ao longo dos epis√≥dios**: Mostra a evolu√ß√£o do Q-valor ao longo dos epis√≥dios para a c√©lula selecionada

<div align="center">
  
![cell](https://github.com/user-attachments/assets/9fa52ad4-bb80-4a75-8f16-a24a83a2dfd0)

</div>

## Algoritmo Q-Learning

O projeto implementa o algoritmo Q-learning cl√°ssico com os seguintes par√¢metros:

- **Œ± (Alpha)**: Taxa de aprendizado - controla o quanto o agente aprende com cada experi√™ncia.
- **Œ≥ (Gamma)**: Fator de desconto - determina a import√¢ncia de recompensas futuras.
- **Œµ (Epsilon)**: Taxa de explora√ß√£o - balanceia explora√ß√£o vs. explota√ß√£o.
- **Epsilon Decay**: Redu√ß√£o gradual da explora√ß√£o ao longo do tempo.

### Estrat√©gia Œµ-greedy
O agente usa uma estrat√©gia epsilon-greedy para balancear:
- **Explora√ß√£o**: Tentar a√ß√µes aleat√≥rias para descobrir novas possibilidades.
- **Explota√ß√£o**: Usar o conhecimento atual para escolher a melhor a√ß√£o conhecida.

## Decis√µes de Design

### Interface do Usu√°rio
- **Design Gaming-Inspired**: Visual moderno com elementos que remetem a jogos cl√°ssicos.
- **Tipografia**: Combina√ß√£o de fontes pixeladas para t√≠tulos e fontes limpas para texto.
- **Paleta de Cores**: Esquema escuro com acentos em roxo para destacar elementos interativos.

### Visualiza√ß√£o de Dados
- **Grid Responsivo**: Layout que se adapta a diferentes tamanhos de mundo.
- **Feedback Visual Imediato**: Mudan√ßas em tempo real refletem as modifica√ß√µes nos par√¢metros.
- **Tooltips Informativos**: Explica√ß√µes contextuais para ajudar na compreens√£o.

### Interatividade
- **Tutorial Integrado**: Modal explicativo para novos usu√°rios.
- **Sugest√µes Contextuais**: Textos que aparecem baseados no estado atual da visualiza√ß√£o.
- **Controles Intuitivos**: Interface familiar similar a players de m√≠dia.

## Desenvolvedores

### Kauan Mariani Ferreira
**GitHub**: [@kauanmaf](https://github.com/kauanmaf)

**Principais Contribui√ß√µes:**
- Desenvolvimento do grid base utilizado em todo o projeto
- Implementa√ß√£o do tutorial interativo
- Configura√ß√£o dos par√¢metros do grid e do agente de aprendizado por refor√ßo (RL)
- Cria√ß√£o do p√¥ster de apresenta√ß√£o
- Defini√ß√£o da paleta de cores e est√©tico da p√°gina

### Pedro Henrique Coterli
**GitHub**: [@PedroPHC25](https://github.com/PedroPHC25)

**Principais Contribui√ß√µes:**
- Implementa√ß√£o do algoritmo de Q-learning
- Gr√°fico de linha da taxa de sucessos ao longo dos epis√≥dios
- Diagrama e gr√°fico de linhas dos Q-valores por c√©lula
- Textos din√¢micos de sugest√µes ao usu√°rio sobre os par√¢metros selecionados


### Gustavo Tironi
**GitHub**: [@gtironi](https://github.com/gtironi)

**Principais Contribui√ß√µes:**
- P√°ginas te√≥ricas (intuitiva, formal e q-leaning)
- Demonstra√ß√£o da atualiza√ß√£o da tabela q-learning
- P√°gina principal e organiza√ß√£o do fluxo
- Design e experi√™ncia do usu√°rio
- Relat√≥rio

## Contexto Acad√™mico

Este projeto foi desenvolvido como trabalho final da disciplina de **Visualiza√ß√£o de Dados** da **Funda√ß√£o Getulio Vargas (FGV)**, com o objetivo de:

- Aplicar t√©cnicas de visualiza√ß√£o de dados para explicar conceitos complexos;
- Criar interfaces interativas para explora√ß√£o de algoritmos de machine learning;
- Demonstrar a aplica√ß√£o pr√°tica de conceitos te√≥ricos de RL.

## Pr√≥ximos Passos

Poss√≠veis melhorias futuras incluem:
- Implementa√ß√£o de outros algoritmos de RL;
- Ambientes mais complexos (gridworlds maiores, obst√°culos din√¢micos);
- Compara√ß√£o entre diferentes algoritmos.
